#!/usr/bin/env python
# -*- coding: utf-8 -*-

from __future__ import (division, print_function, absolute_import,
                        unicode_literals)

__all__ = ["fetch"]

import gevent
from gevent import monkey
monkey.patch_all()

import re
import os
import glob
import gzip
import json
import time
import redis
import shutil
import requests
from datetime import date, timedelta
from tempfile import NamedTemporaryFile

# Make sure that the directory for the local caching of the data exists.
local_data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)),
                              "data")
local_data_dir = os.environ.get("OSRC_DATA_DIR", local_data_dir)
fn_template = os.path.join(local_data_dir,
                           "{year}-{month:02d}-{day:02d}-{n}.json.gz")
try:
    os.makedirs(local_data_dir)
except os.error:
    pass

# The URL template for the GitHub Archive.
archive_url = ("http://data.githubarchive.org/"
               "{year}-{month:02d}-{day:02d}-{n}.json.gz")

# Shared database connection.
redis_pool = redis.ConnectionPool(port=int(os.environ.get("OSRC_REDIS_PORT",
                                                          6380)))

# Regular expression for parsing filename formats.
date_re = re.compile(r"([0-9]{4})-([0-9]{2})-([0-9]{2})-([0-9]+)\.json.gz")


def _fetch_one(year, month, day, n):
    kwargs = {"year": year, "month": month, "day": day, "n": n}
    local_fn = fn_template.format(**kwargs)

    # Skip if the file exists.
    if os.path.exists(local_fn):
        return

    # Download the remote file.
    remote = archive_url.format(**kwargs)
    r = requests.get(remote)
    if r.status_code == requests.codes.ok:
        # Atomically write to disk.
        # http://stackoverflow.com/questions/2333872/ \
        #        atomic-writing-to-file-with-python
        f = NamedTemporaryFile("wb", delete=False)
        f.write(r.content)
        f.flush()
        os.fsync(f.fileno())
        f.close()
        shutil.move(f.name, local_fn)


def fetch(year, month, day):
    """
    Asynchronously download all the event archives for one day of activity
    from the GitHub Archive.

    :param year: The 4-digit year of the date.
    :param month: The integer id of the target month ``[1, 12]``.
    :param day: The integer id of the target day ``[1, 31]``.

    """
    jobs = [gevent.spawn(_fetch_one, year, month, day, n) for n in range(24)]
    gevent.joinall(jobs)


def process(filename):
    """
    Process a single gzipped archive file and push the results to the database.

    :param filename: The absolute path to the archive file.

    """
    # Figure out the day of the week from the filename (this is probably not
    # always right but it'll do).
    year, month, day, hour = map(int, date_re.findall(filename)[0])
    weekday = date(year=year, month=month, day=day).strftime("%w")

    # Set up a redis pipeline.
    r = redis.Redis(connection_pool=redis_pool)
    pipe = r.pipeline(transaction=False)

    # Unzip and load the file.
    strt = time.time()
    with gzip.GzipFile(filename) as f:
        # One event per line.
        events = [line.decode("utf-8") for line in f]

        # One event per line.
        for n, line in enumerate(events):
            # Parse the JSON of this event.
            try:
                event = json.loads(line)
            except:
                print("Failed on line {0} of {1}-{2:02d}-{3:02d}-{4}"
                      .format(n, year, month, day, hour))
                continue

            # Get the user involved and skip if there isn't one.
            actor = event["actor"]
            attrs = event.get("actor_attributes", {})
            if actor is None or attrs.get("type") != "User":
                # This was probably an anonymous event (like a gist event)
                # or an organization event.
                continue

            # Normalize the user name.
            key = actor.lower()

            # Get the type of event.
            evttype = event["type"]
            nevents = 1

            # Can this be called a "contribution"?
            contribution = evttype in ["IssuesEvent", "PullRequestEvent",
                                       "PushEvent"]

            # Increment the global sum histograms.
            pipe.incr("gh:total", nevents)
            pipe.hincrby("gh:day", weekday, nevents)
            pipe.hincrby("gh:hour", hour, nevents)
            pipe.zincrby("gh:user", key, nevents)
            pipe.zincrby("gh:event", evttype, nevents)

            # Event histograms.
            pipe.hincrby("gh:event:{0}:day".format(evttype), weekday, nevents)
            pipe.hincrby("gh:event:{0}:hour".format(evttype), hour, nevents)

            # User schedule histograms.
            pipe.hincrby("gh:user:{0}:day".format(key), weekday, nevents)
            pipe.hincrby("gh:user:{0}:hour".format(key), hour, nevents)

            # User event type histogram.
            pipe.zincrby("gh:user:{0}:event".format(key), evttype, nevents)
            pipe.hincrby("gh:user:{0}:event:{1}:day".format(key, evttype),
                         weekday, nevents)
            pipe.hincrby("gh:user:{0}:event:{1}:hour".format(key, evttype),
                         hour, nevents)

            # Parse the name and owner of the affected repository.
            repo = event.get("repository", {})
            owner, name, org = (repo.get("owner"), repo.get("name"),
                                repo.get("organization"))
            if owner and name:
                repo_name = "{0}/{1}".format(owner, name)
                pipe.zincrby("gh:repo", repo_name, nevents)

                # Is the user contributing to their own project.
                okey = owner.lower()
                if okey == key:
                    pipe.zincrby("gh:user:{0}:repo".format(key),
                                 repo_name, nevents)

                # If not, save all sorts of goodies.
                else:
                    if contribution:
                        pipe.zincrby("gh:contribution", repo_name, nevents)
                        pipe.zincrby("gh:user:{0}:contribution".format(key),
                                     repo_name, nevents)

                    if org is None:
                        # how connected are these two users?
                        pipe.zincrby("gh:user:{0}:connection".format(key),
                                     owner, nevents)
                        pipe.zincrby("gh:user:{0}:connection".format(okey),
                                     actor, nevents)

                        # update the global count of connections.
                        pipe.incr("gh:connection", nevents)
                        pipe.zincrby("gh:connection:user", key, nevents)
                        pipe.zincrby("gh:connection:user", okey, nevents)

                # Do we know what the language of the repository is?
                language = repo.get("language")
                if language:
                    # Which are the most popular languages?
                    pipe.zincrby("gh:lang", language, nevents)

                    # Total number of pushes.
                    if evttype == "PushEvent":
                        pipe.zincrby("gh:pushes:lang", language, nevents)

                    # What are a user's favorite languages?
                    if contribution:
                        pipe.zincrby("gh:user:{0}:lang".format(key), language,
                                     nevents)

                    # Who are the most important users of a language?
                    if contribution:
                        pipe.zincrby("gh:lang:{0}:user".format(language),
                                     key, nevents)

        pipe.execute()

        print("Processed {0} events in {1} [{2:.2f} seconds]"
              .format(n, filename, time.time() - strt))


def fetch_and_process(year, month, day):
    fetch(year, month, day)
    kwargs = {"year": year, "month": month, "day": day, "n": "*"}
    filenames = glob.glob(fn_template.format(**kwargs))
    if len(filenames) != 24:
        print("Missing {0} archive files for date {1:04d}-{2:02d}-{3:02d}"
              .format(24 - len(filenames), year, month, day))
    map(process, filenames)


if __name__ == "__main__":
    yesterday = date.today() - timedelta(1)
    fetch_and_process(yesterday.year, yesterday.month, yesterday.day)
